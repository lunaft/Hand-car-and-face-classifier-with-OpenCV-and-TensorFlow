{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-17 11:25:47.419121: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 600 images belonging to 2 classes.\n",
      "Found 600 images belonging to 2 classes.\n",
      "Epoch 1/20\n",
      "299/299 [==============================] - 147s 488ms/step - loss: 0.8402 - accuracy: 0.7458 - val_loss: 1.3137 - val_accuracy: 0.6890\n",
      "Epoch 2/20\n",
      "299/299 [==============================] - 148s 496ms/step - loss: 0.3469 - accuracy: 0.8729 - val_loss: 0.9917 - val_accuracy: 0.6689\n",
      "Epoch 3/20\n",
      "299/299 [==============================] - 145s 484ms/step - loss: 0.3079 - accuracy: 0.8963 - val_loss: 1.2165 - val_accuracy: 0.6722\n",
      "Epoch 4/20\n",
      "299/299 [==============================] - 143s 477ms/step - loss: 0.3618 - accuracy: 0.8796 - val_loss: 1.1611 - val_accuracy: 0.7191\n",
      "Epoch 5/20\n",
      "299/299 [==============================] - 133s 445ms/step - loss: 0.3541 - accuracy: 0.8863 - val_loss: 1.5459 - val_accuracy: 0.6789\n",
      "Epoch 6/20\n",
      "299/299 [==============================] - 132s 442ms/step - loss: 0.1333 - accuracy: 0.9532 - val_loss: 1.2619 - val_accuracy: 0.6823\n",
      "Epoch 7/20\n",
      "299/299 [==============================] - 132s 442ms/step - loss: 0.1690 - accuracy: 0.9532 - val_loss: 1.5832 - val_accuracy: 0.6656\n",
      "Epoch 8/20\n",
      "299/299 [==============================] - 133s 445ms/step - loss: 0.2527 - accuracy: 0.9164 - val_loss: 2.7904 - val_accuracy: 0.6187\n",
      "Epoch 9/20\n",
      "299/299 [==============================] - 136s 454ms/step - loss: 0.1591 - accuracy: 0.9465 - val_loss: 1.7375 - val_accuracy: 0.6656\n",
      "Epoch 10/20\n",
      "299/299 [==============================] - 134s 449ms/step - loss: 0.2153 - accuracy: 0.9398 - val_loss: 0.8345 - val_accuracy: 0.7258\n",
      "Epoch 11/20\n",
      "299/299 [==============================] - 139s 464ms/step - loss: 0.1651 - accuracy: 0.9498 - val_loss: 1.7599 - val_accuracy: 0.6990\n",
      "Epoch 12/20\n",
      "299/299 [==============================] - 141s 472ms/step - loss: 0.1571 - accuracy: 0.9565 - val_loss: 1.8434 - val_accuracy: 0.6689\n",
      "Epoch 13/20\n",
      "299/299 [==============================] - 135s 451ms/step - loss: 0.0750 - accuracy: 0.9766 - val_loss: 1.3717 - val_accuracy: 0.7291\n",
      "Epoch 14/20\n",
      "299/299 [==============================] - 134s 449ms/step - loss: 0.1304 - accuracy: 0.9465 - val_loss: 2.3544 - val_accuracy: 0.6923\n",
      "Epoch 15/20\n",
      "299/299 [==============================] - 132s 441ms/step - loss: 0.1591 - accuracy: 0.9532 - val_loss: 1.7990 - val_accuracy: 0.6522\n",
      "Epoch 16/20\n",
      "299/299 [==============================] - 134s 449ms/step - loss: 0.0662 - accuracy: 0.9699 - val_loss: 1.8222 - val_accuracy: 0.6990\n",
      "Epoch 17/20\n",
      "299/299 [==============================] - 135s 451ms/step - loss: 0.0705 - accuracy: 0.9732 - val_loss: 1.7221 - val_accuracy: 0.6923\n",
      "Epoch 18/20\n",
      "299/299 [==============================] - 134s 448ms/step - loss: 0.0581 - accuracy: 0.9833 - val_loss: 2.9558 - val_accuracy: 0.6421\n",
      "Epoch 19/20\n",
      "299/299 [==============================] - 136s 455ms/step - loss: 0.1513 - accuracy: 0.9565 - val_loss: 1.0698 - val_accuracy: 0.7258\n",
      "Epoch 20/20\n",
      "299/299 [==============================] - 127s 426ms/step - loss: 0.1157 - accuracy: 0.9699 - val_loss: 1.0123 - val_accuracy: 0.7023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Flatten, Dense, Activation, MaxPooling2D\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "training_data_path = '/Users/luna/Desktop/carpeta sin título/hand identifier/photos/test'\n",
    "validation_data_path = '/Users/luna/Desktop/carpeta sin título/hand identifier/photos/train'\n",
    "\n",
    "K.clear_session()  # Clear everything\n",
    "\n",
    "# Parameters\n",
    "iterations = 20  # Number of iterations to adjust our model\n",
    "height, width = 200, 200  # Size of the training images\n",
    "batch_size = 1  # Number of images that we will send sequentially\n",
    "steps = 299 / 1  # Number of times information will be processed in each iteration\n",
    "validation_steps = 299 / 1  # After each iteration, we validate the previous one\n",
    "conv1_filters = 32\n",
    "conv2_filters = 64  # Number of filters that we will apply in each convolution\n",
    "filter_size1 = (3, 3)\n",
    "filter_size2 = (2, 2)  # Sizes of filters 1 and 2\n",
    "pool_size = (2, 2)  # Size of the filter in max pooling\n",
    "classes = 2  # Open and closed hand (5 fingers and 0 fingers)\n",
    "lr = 0.0005  # Adjustments of the neural network to approach an optimal solution\n",
    "\n",
    "# Pre-Processing of the images\n",
    "training_preprocessing = ImageDataGenerator(\n",
    "    rescale=1. / 255,  # Scale pixels from 0 to 255 | 0 to 1\n",
    "    shear_range=0.3,  # Generate our images tilted for better training\n",
    "    zoom_range=0.3,  # Generates images with zoom for better training\n",
    "    horizontal_flip=True  # Flip the images for better training\n",
    ")\n",
    "\n",
    "validation_preprocessing = ImageDataGenerator(\n",
    "    rescale=1. / 255\n",
    ")\n",
    "\n",
    "training_image = training_preprocessing.flow_from_directory(\n",
    "    training_data_path,  # It will take the photos that we have already stored\n",
    "    target_size=(height, width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical',  # Categorical classification = by classes\n",
    ")\n",
    "\n",
    "validation_image = validation_preprocessing.flow_from_directory(\n",
    "    validation_data_path,\n",
    "    target_size=(height, width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Create the Convolutional Neural Network (CNN)\n",
    "cnn = Sequential()  # Sequential neural network\n",
    "# We add filters in order to make our image very deep but small\n",
    "cnn.add(Conv2D(conv1_filters, filter_size1, padding='same', input_shape=(height, width, 3), activation='relu'))  # Add the first layer\n",
    "cnn.add(MaxPooling2D(pool_size=pool_size))  # After the first layer, we will have a max pooling layer and assign the size\n",
    "\n",
    "cnn.add(Conv2D(conv2_filters, filter_size2, padding='same', activation='relu'))  # Add new layer\n",
    "\n",
    "cnn.add(MaxPooling2D(pool_size=pool_size))\n",
    "\n",
    "# Now we will convert that deep image into a flat one, to have 1 dimension with all the info\n",
    "cnn.add(Flatten())  # Flatten the image\n",
    "cnn.add(Dense(256, activation='relu'))  # Assign 256 neurons\n",
    "cnn.add(Dropout(0.5))  # Turn off 50% of the neurons in the previous function to avoid overfitting\n",
    "cnn.add(Dense(classes, activation='softmax'))  # It's our last layer, it tells us the probability of being one of the classes\n",
    "\n",
    "# Add parameters to optimize the model\n",
    "# During training have a self-evaluation, optimize with Adam, and the metric will be accuracy\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "cnn.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# We will train our network\n",
    "cnn.fit(training_image, steps_per_epoch=steps, epochs=iterations, validation_data=validation_image, validation_steps=validation_steps)\n",
    "\n",
    "# Save the model\n",
    "cnn.save('Model.h5')\n",
    "cnn.save_weights('weights.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5 (v3.11.5:cce6ba91b3, Aug 24 2023, 10:50:31) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
